# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains data about marketing campaign of a Portuguese banking institution and we seek to predict if the client will subscribe a term deposit (variable y).
There are 20 input features. They are:

1- age (numeric)
2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)
4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')
5 - default: has credit in default? (categorical: 'no','yes','unknown')
6 - housing: has housing loan? (categorical: 'no','yes','unknown')
7 - loan: has personal loan? (categorical: 'no','yes','unknown')
8 - contact: contact communication type (categorical: 'cellular','telephone')
9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.
12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
14 - previous: number of contacts performed before this campaign and for this client (numeric)
15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)
17 - cons.price.idx: consumer price index - monthly indicator (numeric)
18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)
19 - euribor3m: euribor 3 month rate - daily indicator (numeric)
20 - nr.employed: number of employees - quarterly indicator (numeric)

The target is the 'y' column which answers wether the client subscribed a term deposit? (binary: 'yes','no')


**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was one with score of 0.9117

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
Train.py
Tabular Dataset➡ Sklearn Logistic Regression➡  HyperDrive➡  HyperDrive Model➡  Report
Train.py
Tabular Dataset➡ Sklearn Logistic Regression➡  Tabular Dataset➡  AutoMl➡  AutoMl Model➡ Report

The classification algorithm is Logistics Regression which is a Binary classification algorithm.

primary_metric_config':"name": "Accuracy", "goal": "maximize
The data is a marketing data that has a shape of 32950, 21
Random sampling, which supports discrete and continuous hyperparameters was used
The primary metric to optimize was accuracy and the the goal was to maximize.
Early termination policy was Bandit Policy and the parameter were slack_factor and evaluation_interval.
The algorithm is a two class classification to predict between two categories.

In the Ramdom parameter sampling, the specified hyperparameters are
C and max_iter
## Parameters generated by AutoML
min_samples_split=0.2442105263157895,
min_weight_fraction_leaf=0.0,
n_estimators=10,
n_jobs=1,
oob_score=False,
random_state=None,
verbose=0,
warm_start=False

**What are the benefits of the parameter sampler you chose?**
In random sampling, hyperparameter values are randomly selected from the defined search space.
It supports early termination of low-performance runs.
This option is useful for cases where you want to increase model performance using the metrics of your choice but still conserve computing resources.

**What are the benefits of the early stopping policy you chose?**
The specified parameters are slack_factor(The ratio used to calculate the allowed distance from the best performing experiment run) and evaluation_interval(The frequency for applying the policy). Here the benefits could be seen in that any run that doesn't fall within the slack factor of the evaluation metric with respect to the best performing run will be terminated. This saves appreciable time and resources.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
Ensemble methods helps improve machine learning results by combining multiple models. I believe using ensemble methods allowed to produce better predictions compared to a single model.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
hyper parameter tuning with hyperdrive gave accuracy of 0.9115 while AML best performing model was 0.9171.
This difference is because of AML's ability to iteratively find the best hyperparameter values for the model.
## Best Model generated by AutoML
StackEnsemble 
## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Bias in input data towards one class can affect the model accuracy. Handling this will be a huge improvement
## Improving AutoML
follow ML best-practices by:
Using more training data, and eliminating statistical bias
Preventing target leakage
Using fewer features
Regularization and hyperparameter optimization
Model complexity limitations
Cross-validation
Avoiding Target leakage
Removing features

## Improving SKlearn
Trying more hyperparameters
Using cross-validation to reduce over-fitting or choosing other robust models etc.
## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**

